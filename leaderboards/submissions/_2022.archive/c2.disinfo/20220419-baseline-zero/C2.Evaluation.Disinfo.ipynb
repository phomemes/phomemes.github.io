{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbbb6245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# PhoMemes 2022 Evaluation Script\n",
    "#  - For Disinformation Challenge Evaluation\n",
    "#\n",
    "# Used to evaluate PhoMeme challenge submission for\n",
    "#  for \n",
    "# --------------------------------------------------\n",
    "version = 1.0 # Notebook Version Number\n",
    "edition = \"c2\"\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34744aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "863e6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f50f160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63892d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runtag': 'zero.rule',\n",
       " 'date': '2022-05-12',\n",
       " 'organization': 'umd',\n",
       " 'model_description': 'Baseline that predicts the most common label, which is no disinformation agent.',\n",
       " 'type': 'automatic',\n",
       " 'paper': '',\n",
       " 'code': ''}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info = None\n",
    "with open(\"metadata.json\", \"r\") as in_file:\n",
    "    model_info = json.load(in_file)\n",
    "    \n",
    "model_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790842a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d792cd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_df = pd.read_csv(\"../../../data/c2.disinfo/groundtruth.csv\", index_col=\"user_id\")\n",
    "run_df = pd.read_csv(\"run.csv\", index_col=\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1824cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "866d68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = run_df.join(groundtruth_df, how=\"inner\", lsuffix=\"_pred\", rsuffix=\"_truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc8998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3323c510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['authentic_pred', 'campaign_pred', 'authentic_truth', 'campaign_truth'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a54bb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cbuntain/Development/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "disinfo_agent_prediction = [True if a == 0 else False for a in joined_df[\"authentic_pred\"]]\n",
    "disinfo_agent_truth = [True if a == 0 else False for a in joined_df[\"authentic_truth\"]]\n",
    "\n",
    "ac_score_overall = accuracy_score(disinfo_agent_prediction, disinfo_agent_truth)\n",
    "pr_score_overall = precision_score(disinfo_agent_prediction, disinfo_agent_truth)\n",
    "rc_score_overall = recall_score(disinfo_agent_prediction, disinfo_agent_truth)\n",
    "\n",
    "f1_score_overall = f1_score(disinfo_agent_prediction, disinfo_agent_truth, zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d982b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "211a9aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4309656823623304\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", ac_score_overall)\n",
    "print(\"Precision:\", pr_score_overall)\n",
    "print(\"Recall:\", rc_score_overall)\n",
    "print(\"F1:\", f1_score_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897ed27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ce6d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None 1080\n",
      "[None] Ac: 0.4309656823623304\n",
      "[None] Pr: 1.0\n",
      "[None] Rc: 0.4309656823623304\n",
      "[None] F1: 0.6023424428332403\n",
      "********************\n",
      "campaign.venezuela_2019 454\n",
      "[campaign.venezuela_2019] Ac: 0.8188347964884277\n",
      "[campaign.venezuela_2019] Pr: 0.0\n",
      "[campaign.venezuela_2019] Rc: 0.0\n",
      "[campaign.venezuela_2019] F1: 0.0\n",
      "********************\n",
      "campaign.iranian_2018 395\n",
      "[campaign.iranian_2018] Ac: 0.8423782920989625\n",
      "[campaign.iranian_2018] Pr: 0.0\n",
      "[campaign.iranian_2018] Rc: 0.0\n",
      "[campaign.iranian_2018] F1: 0.0\n",
      "********************\n",
      "campaign.russia_2018 336\n",
      "[campaign.russia_2018] Ac: 0.8659217877094972\n",
      "[campaign.russia_2018] Pr: 0.0\n",
      "[campaign.russia_2018] Rc: 0.0\n",
      "[campaign.russia_2018] F1: 0.0\n",
      "********************\n",
      "campaign.china_2019 241\n",
      "[campaign.china_2019] Ac: 0.903830806065443\n",
      "[campaign.china_2019] Pr: 0.0\n",
      "[campaign.china_2019] Rc: 0.0\n",
      "[campaign.china_2019] F1: 0.0\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "acs = []\n",
    "prs = []\n",
    "rcs = []\n",
    "f1s = []\n",
    "\n",
    "for i,j in joined_df[\"campaign_truth\"].value_counts().items():\n",
    "    print(i,j)\n",
    "    \n",
    "    this_pred = joined_df['campaign_pred'] == i\n",
    "    this_true = joined_df['campaign_truth'] == i\n",
    "    \n",
    "    ac_score_this = accuracy_score(this_pred, this_true)\n",
    "    acs.append(ac_score_this)\n",
    "    \n",
    "    pr_score_this = precision_score(this_pred, this_true, zero_division=0)\n",
    "    prs.append(pr_score_this)\n",
    "    \n",
    "    rc_score_this = recall_score(this_pred, this_true, zero_division=0)\n",
    "    rcs.append(rc_score_this)\n",
    "    \n",
    "    f1_score_this = f1_score(this_pred, this_true, zero_division=0)\n",
    "    f1s.append(f1_score_this)\n",
    "    \n",
    "    print(\"[%s] Ac:\" % i, ac_score_this)\n",
    "    print(\"[%s] Pr:\" % i, pr_score_this)\n",
    "    print(\"[%s] Rc:\" % i, rc_score_this)\n",
    "    print(\"[%s] F1:\" % i, f1_score_this)\n",
    "    \n",
    "    print(\"*\" * 20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177760f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17f8bf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-Average Ac: 0.7723862729449322\n",
      "Macro-Average Pr: 0.2\n",
      "Macro-Average Rc: 0.08619313647246608\n",
      "Macro-Average F1: 0.12046848856664807\n"
     ]
    }
   ],
   "source": [
    "ac_mean = np.mean(acs)\n",
    "pr_mean = np.mean(prs)\n",
    "rc_mean = np.mean(rcs)\n",
    "f1_mean = np.mean(f1s)\n",
    "\n",
    "print(\"Macro-Average Ac:\", ac_mean)\n",
    "print(\"Macro-Average Pr:\", pr_mean)\n",
    "print(\"Macro-Average Rc:\", rc_mean)\n",
    "print(\"Macro-Average F1:\", f1_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76002a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85588106",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_date_ = cwd.partition(\"submissions/\")[-1].partition(\"/\")[-1]\n",
    "sub_date_ = sub_date_.partition(\"-\")[0]\n",
    "sub_date = \"%s/%s/%s\" % (sub_date_[:4], sub_date_[4:6], sub_date_[6:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d1c958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a999c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>organization</th>\n",
       "      <th>model_description</th>\n",
       "      <th>type</th>\n",
       "      <th>paper</th>\n",
       "      <th>code</th>\n",
       "      <th>accuracy_overall</th>\n",
       "      <th>precision_overall</th>\n",
       "      <th>recall_overall</th>\n",
       "      <th>f1_overall</th>\n",
       "      <th>accuracy_macro</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>runtag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>umd-zero.rule-2022-05-12</th>\n",
       "      <td>2022/04/19</td>\n",
       "      <td>umd</td>\n",
       "      <td>Baseline that predicts the most common label, ...</td>\n",
       "      <td>automatic</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.430966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.772386</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.086193</td>\n",
       "      <td>0.120468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date organization  \\\n",
       "runtag                                              \n",
       "umd-zero.rule-2022-05-12  2022/04/19          umd   \n",
       "\n",
       "                                                          model_description  \\\n",
       "runtag                                                                        \n",
       "umd-zero.rule-2022-05-12  Baseline that predicts the most common label, ...   \n",
       "\n",
       "                               type paper code  accuracy_overall  \\\n",
       "runtag                                                             \n",
       "umd-zero.rule-2022-05-12  automatic                     0.430966   \n",
       "\n",
       "                          precision_overall  recall_overall  f1_overall  \\\n",
       "runtag                                                                    \n",
       "umd-zero.rule-2022-05-12                0.0             0.0         0.0   \n",
       "\n",
       "                          accuracy_macro  precision_macro  recall_macro  \\\n",
       "runtag                                                                    \n",
       "umd-zero.rule-2022-05-12        0.772386              0.2      0.086193   \n",
       "\n",
       "                          f1_macro  \n",
       "runtag                              \n",
       "umd-zero.rule-2022-05-12  0.120468  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = [{\n",
    "    \"runtag\": \"%s-%s-%s\" % (model_info[\"organization\"], model_info[\"runtag\"], model_info[\"date\"]),\n",
    "    \"date\": sub_date,\n",
    "    \"organization\": model_info[\"organization\"],\n",
    "    \"model_description\": model_info[\"model_description\"],\n",
    "    \"type\": model_info[\"type\"],\n",
    "    \"paper\": model_info[\"paper\"],\n",
    "    \"code\": model_info[\"code\"],\n",
    "    \"accuracy_overall\": ac_score_overall,\n",
    "    \"precision_overall\": pr_score_overall,\n",
    "    \"recall_overall\": rc_score_overall,\n",
    "    \"f1_overall\": f1_score_overall,\n",
    "    \"accuracy_macro\": ac_mean,\n",
    "    \"precision_macro\": pr_mean,\n",
    "    \"recall_macro\": rc_mean,\n",
    "    \"f1_macro\": f1_mean,\n",
    "}]\n",
    "\n",
    "leaderboard_df = pd.DataFrame(rows).set_index(\"runtag\")\n",
    "leaderboard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceb8a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74440688",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_df.to_csv(\"leaderboard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d747fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
